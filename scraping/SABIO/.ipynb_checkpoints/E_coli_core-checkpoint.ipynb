{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine BiGG IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas\n",
    "import json, re\n",
    "\n",
    "# define the reaction translation\n",
    "# reactions = pandas.read_table(open('BiGG_reactions.txt', 'r'), sep = '\\t')\n",
    "# reactions_dict = {}\n",
    "# for index, met in reactions.iterrows():\n",
    "#     reactions_dict[met['bigg_id']] = {\n",
    "#         'name':met['name'],\n",
    "#         'reaction_string':met['reaction_string']\n",
    "#     }\n",
    "# with open('BiGG_reactions, parsed.json', 'w') as out:\n",
    "#     json.dump(reactions_dict, out, indent = 2)\n",
    "\n",
    "# define the metabolite translation: BiGG -> SABIO\n",
    "# metabolites = pandas.read_table(open('BiGG_metabolites.txt', 'r'), sep = '\\t')\n",
    "# metabolites_dict = {}\n",
    "# for index, met in metabolites.iterrows():\n",
    "#     bigg_name = str(met['name'])\n",
    "#     bigg_name = bigg_name.strip()\n",
    "#     bigg_id = met['universal_bigg_id']\n",
    "    \n",
    "#     name = re.sub('\\s[A-Z0-9]{3,}$', '', str(met['name']))\n",
    "#     name = name.strip()\n",
    "#     metabolites_dict[bigg_id] = {\n",
    "#         'name':name\n",
    "#     }\n",
    "#     if bigg_name != name:\n",
    "#         metabolites_dict[bigg_id]['bigg_name'] = bigg_name\n",
    "        \n",
    "# with open('BiGG_metabolites, parsed.json', 'w') as out:\n",
    "#     json.dump(metabolites_dict, out, indent = 2)\n",
    "    \n",
    "    \n",
    "# define the metabolite translation: SABIO -> BiGG\n",
    "metabolites = pandas.read_table(open('BiGG_metabolites.txt', 'r'), sep = '\\t')\n",
    "metabolites_dict = {}\n",
    "for index, met in metabolites.iterrows():\n",
    "    bigg_name = str(met['name'])\n",
    "    bigg_name = bigg_name.strip()\n",
    "    bigg_id = met['universal_bigg_id']\n",
    "    \n",
    "    name = re.sub('\\s[A-Z0-9]{3,}$', '', str(met['name']))\n",
    "    name = name.strip()\n",
    "    metabolites_dict[name] = {\n",
    "        'id':bigg_id\n",
    "    }\n",
    "    if bigg_name != name:\n",
    "        metabolites_dict[name]['bigg_name'] = bigg_name\n",
    "        \n",
    "with open('BiGG_metabolite_names, parsed.json', 'w') as out:\n",
    "    json.dump(metabolites_dict, out, indent = 2)\n",
    "    \n",
    "# pprint(metabolites_dict)\n",
    "# pprint(reactions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine the BiGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_reaction(reaction_string, bigg_metabolites):\n",
    "    def _parse_stoich(met):\n",
    "        stoich = ''\n",
    "        ch_number = 0\n",
    "        denom = False\n",
    "        while re.search('[0-9\\./]', met[ch_number]): \n",
    "            stoich += met[ch_number]\n",
    "            if met[ch_number] == '/':\n",
    "                numerator = stoich\n",
    "                denom = True\n",
    "            if denom:\n",
    "                denominator += met[ch_number]\n",
    "            ch_number += 1\n",
    "            \n",
    "        if denom:\n",
    "            stoich = f'{numerator}/{denominator}'\n",
    "        return stoich\n",
    "    \n",
    "    def met_parsing(met):\n",
    "#         print(met)\n",
    "        met = met.strip()\n",
    "        if re.search('(\\d\\s\\w|\\d\\.\\d\\s|\\d/\\d\\s)', met):\n",
    "            coefficient = _parse_stoich(met)\n",
    "            coefficient = '{} '.format(coefficient)\n",
    "        else:\n",
    "            coefficient = ''\n",
    "        met = re.sub(coefficient, '', met)\n",
    "#         print(met, coefficient)\n",
    "        return met, coefficient   \n",
    "\n",
    "    def reformat_met_name(met_name, sabio = False):\n",
    "        met_name = re.sub(' - ', '-', met_name)\n",
    "        if not sabio:\n",
    "            met_name = re.sub(' ', '_', met_name)\n",
    "        return met_name\n",
    "    \n",
    "        \n",
    "    # parse the reactants and products for the specified reaction string\n",
    "    reaction_split = reaction_string.split('<->')\n",
    "    reactants_list = reaction_split[0].split(' + ')\n",
    "    products_list = reaction_split[1].split(' + ')\n",
    "    \n",
    "    # parse the reactants\n",
    "    reactants = []\n",
    "    sabio_reactants = []\n",
    "    for met in reactants_list:\n",
    "#         print(met)\n",
    "        met = met.strip()\n",
    "        met = re.sub('_\\w$', '', met)\n",
    "        met, coefficient = met_parsing(met)\n",
    "        reactants.append(coefficient + reformat_met_name(bigg_metabolites[met]['name']))\n",
    "        sabio_reactants.append(coefficient + reformat_met_name(bigg_metabolites[met]['name'], True))\n",
    "\n",
    "    # parse the products\n",
    "    products = []\n",
    "    sabio_products = []\n",
    "    for met in products_list:\n",
    "        if not re.search('[a-z]', met, flags = re.IGNORECASE):\n",
    "            continue\n",
    "        met = met.strip()\n",
    "        met = re.sub('_\\w$', '', met)\n",
    "        met, coefficient = met_parsing(met)\n",
    "        products.append(coefficient + reformat_met_name(bigg_metabolites[met]['name']))\n",
    "        sabio_products.append(coefficient + reformat_met_name(bigg_metabolites[met]['name'], True))\n",
    "\n",
    "#     compounds = reactants + products\n",
    "    reactant_string = ' + '.join(reactants)\n",
    "    product_string = ' + '.join(products)\n",
    "    reaction_string = ' <-> '.join([reactant_string, product_string])\n",
    "    \n",
    "    # construct the set of compounds in the SABIO format\n",
    "    sabio_compounds = sabio_reactants + sabio_products\n",
    "    \n",
    "    return reaction_string, sabio_compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas\n",
    "import json\n",
    "%run ../dfbapy/dfba.py\n",
    "\n",
    "bigg_reactions = json.load(open('BiGG_reactions, parsed.json'))\n",
    "bigg_metabolites = json.load(open('BiGG_metabolites, parsed.json'))\n",
    "\n",
    "# substitute the reaction and metabolite names\n",
    "model = json.load(open('Ecoli core, BiGG, indented.json'))\n",
    "model_contents = {}\n",
    "for reaction in model['reactions']:\n",
    "    # define the reaction identification\n",
    "    reaction_id = reaction['id'] \n",
    "    reaction_name = bigg_reactions[reaction_id]['name']\n",
    "    \n",
    "    # substitute the reaction string\n",
    "    og_reaction_string = bigg_reactions[reaction_id]['reaction_string']\n",
    "#     print('\\n\\n', og_reaction_string)\n",
    "    reaction_string, compounds = split_reaction(og_reaction_string, bigg_metabolites)\n",
    "#     print(reaction_string)\n",
    "\n",
    "    model_contents[reaction_name] = {\n",
    "        'reaction': {\n",
    "            'original': og_reaction_string,\n",
    "            'substituted': reaction_string,\n",
    "        },\n",
    "        'chemicals': compounds,\n",
    "        'annotations': reaction['annotation']\n",
    "    }\n",
    "    \n",
    "# pprint(model_contents)\n",
    "with open('processed_Ecoli_model.json', 'w') as out:\n",
    "    json.dump(model_contents, out, indent = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine the data combination function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json, re\n",
    "\n",
    "class CaseInsensitiveDict(dict):\n",
    "    @classmethod\n",
    "    def _k(cls, key):\n",
    "        return key.lower() if isinstance(key, str) else key\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CaseInsensitiveDict, self).__init__(*args, **kwargs)\n",
    "        self._convert_keys()\n",
    "    def __getitem__(self, key):\n",
    "        return super(CaseInsensitiveDict, self).__getitem__(self.__class__._k(key))\n",
    "    def __setitem__(self, key, value):\n",
    "        super(CaseInsensitiveDict, self).__setitem__(self.__class__._k(key), value)\n",
    "    def __delitem__(self, key):\n",
    "        return super(CaseInsensitiveDict, self).__delitem__(self.__class__._k(key))\n",
    "    def __contains__(self, key):\n",
    "        return super(CaseInsensitiveDict, self).__contains__(self.__class__._k(key))\n",
    "    def has_key(self, key):\n",
    "        return super(CaseInsensitiveDict, self).has_key(self.__class__._k(key))\n",
    "    def pop(self, key, *args, **kwargs):\n",
    "        return super(CaseInsensitiveDict, self).pop(self.__class__._k(key), *args, **kwargs)\n",
    "    def get(self, key, *args, **kwargs):\n",
    "        return super(CaseInsensitiveDict, self).get(self.__class__._k(key), *args, **kwargs)\n",
    "    def setdefault(self, key, *args, **kwargs):\n",
    "        return super(CaseInsensitiveDict, self).setdefault(self.__class__._k(key), *args, **kwargs)\n",
    "    def update(self, E=None, **F):\n",
    "        super(CaseInsensitiveDict, self).update(self.__class__(E))\n",
    "        super(CaseInsensitiveDict, self).update(self.__class__(**F))\n",
    "    def _convert_keys(self):\n",
    "        for k in list(self.keys()):\n",
    "            v = super(CaseInsensitiveDict, self).pop(k)\n",
    "            self.__setitem__(k, v)\n",
    "\n",
    "class test():\n",
    "    def __init__(self):\n",
    "        self.sabio_df = pandas.read_csv(open('proccessed-xls.csv', encoding=\"utf-8\"))\n",
    "        self.bigg_to_sabio_metabolites = json.load(open('BiGG_metabolites, parsed.json'))\n",
    "        self.sabio_to_bigg_metabolites = json.load(open('BiGG_metabolite_names, parsed.json'))\n",
    "        self.bigg_reactions = json.load(open('BiGG_reactions, parsed.json'))\n",
    "        \n",
    "        self.paths = {}\n",
    "        self.paths['entryids_path'] = 'entryids_progress.json'\n",
    "        self.paths[\"scraped_model_path\"] = 'scraped_model.json'\n",
    "        \n",
    "        self.printing = True\n",
    "        \n",
    "    def _split_reaction(self, \n",
    "                        reaction_string, # the sabio or bigg reaction string\n",
    "                        sabio = False   # specifies how the reaction string will be split\n",
    "                        ):\n",
    "        def _parse_stoich(met):\n",
    "            stoich = ''\n",
    "            ch_number = 0\n",
    "            denom = False\n",
    "            numerator = denominator = 0\n",
    "            while re.search('[0-9\\./]', met[ch_number]): \n",
    "                stoich += met[ch_number]\n",
    "                if met[ch_number] == '/':\n",
    "                    numerator = stoich\n",
    "                    denom = True\n",
    "                if denom:\n",
    "                    denominator += met[ch_number]\n",
    "                ch_number += 1\n",
    "                \n",
    "            if denom:\n",
    "                stoich = f'{numerator}/{denominator}'\n",
    "            return stoich\n",
    "        \n",
    "        def met_parsing(met):\n",
    "    #         print(met)\n",
    "            met = met.strip()\n",
    "            met = re.sub('_\\w$', '', met)\n",
    "            if re.search('(\\d\\s\\w|\\d\\.\\d\\s|\\d/\\d\\s)', met):\n",
    "                coefficient = _parse_stoich(met)\n",
    "                coefficient = '{} '.format(coefficient)\n",
    "            else:\n",
    "                coefficient = ''\n",
    "            met = re.sub(coefficient, '', met)\n",
    "    #         print(met, coefficient)\n",
    "            return met, coefficient   \n",
    "    \n",
    "        def reformat_met_name(met_name, sabio = False):\n",
    "            met_name = re.sub(' - ', '-', met_name)\n",
    "            return met_name\n",
    "        \n",
    "        def parsing_chemical_list(chemical_list):\n",
    "            bigg_chemicals = []\n",
    "            sabio_chemicals = []\n",
    "            for met in chemical_list:\n",
    "#                 print('metabolite', met, type(met))\n",
    "                if not re.search('[A-Za-z]', met):\n",
    "                    continue\n",
    "                met, coefficient = met_parsing(met)\n",
    "                # assign the proper chemical names\n",
    "                if not sabio:\n",
    "                    sabio_chemicals.append(coefficient + reformat_met_name(self.bigg_to_sabio_metabolites[met]['name'], True))     \n",
    "                    if 'bigg_name' in self.bigg_to_sabio_metabolites[met]:\n",
    "                        bigg_chemicals.append(coefficient + reformat_met_name(self.bigg_to_sabio_metabolites[met]['bigg_name']))\n",
    "                    else:\n",
    "                        bigg_chemicals.append(coefficient + reformat_met_name(self.bigg_to_sabio_metabolites[met]['name']))\n",
    "                    \n",
    "                elif sabio:\n",
    "                    sabio_chemicals.append(coefficient + reformat_met_name(met, True))   \n",
    "#                     if met in list(self.sabio_to_bigg_metabolites.keys()):\n",
    "#                         print('yes')\n",
    "#                     try:\n",
    "#                         print(self.sabio_to_bigg_metabolites[met])\n",
    "#                     except:\n",
    "# #                         print(met, '\\n\\n\\n', self.sabio_to_bigg_metabolites.keys())\n",
    "#                         for ch in met:\n",
    "#                             print(ch, '\\t', ord(ch))\n",
    "            \n",
    "                    dic = CaseInsensitiveDict(self.sabio_to_bigg_metabolites)\n",
    "                    if 'bigg_name' in dic.get(met):\n",
    "                        bigg_chemicals.append(coefficient + reformat_met_name(dic.get(met)['bigg_name']))\n",
    "                    else:\n",
    "                        bigg_chemicals.append(coefficient + reformat_met_name(met))\n",
    "                                              \n",
    "#                 bigg_chemicals.append(coefficient + reformat_met_name(self.bigg_metabolites[met]['name']))\n",
    "#                 sabio_chemicals.append(coefficient + reformat_met_name(self.bigg_metabolites[met]['name'], True))\n",
    "#                 if 'bigg_name' in self.bigg_metabolites[met]:\n",
    "#                     bigg_chemicals[-1] = coefficient + reformat_met_name(self.bigg_metabolites[met]['name'])\n",
    "            \n",
    "            return bigg_chemicals, sabio_chemicals\n",
    "        \n",
    "            \n",
    "        # parse the reactants and products for the specified reaction string\n",
    "        if not sabio:\n",
    "            reaction_split = reaction_string.split(' <-> ')\n",
    "        else:\n",
    "            reaction_split = reaction_string.split(' = ')\n",
    "            \n",
    "        reactants_list = reaction_split[0].split(' + ')\n",
    "        products_list = reaction_split[1].split(' + ')\n",
    "        \n",
    "        # parse the reactants and products\n",
    "        bigg_reactants, sabio_reactants = parsing_chemical_list(reactants_list)\n",
    "        bigg_products, sabio_products = parsing_chemical_list(products_list)\n",
    "        \n",
    "        # assemble the chemicals list and reaction string\n",
    "        bigg_compounds = bigg_reactants + bigg_products\n",
    "        sabio_chemicals = sabio_reactants + sabio_products\n",
    "        reactant_string = ' + '.join(bigg_reactants)\n",
    "        product_string = ' + '.join(bigg_products)\n",
    "        reaction_string = ' <-> '.join([reactant_string, product_string])\n",
    "#        if sabio:\n",
    "#            reaction_string = ' = '.join([reactant_string, product_string])        \n",
    "        \n",
    "        return reaction_string, sabio_chemicals, bigg_compounds\n",
    "        \n",
    "    def combine_data(self,):\n",
    "        # import previously parsed content\n",
    "        self.model_contents = json.load(open('processed_Ecoli_model.json'))\n",
    "        with open(self.paths['entryids_path']) as json_file: \n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        # combine the scraped data into a programmable JSON  \n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        enzymes = self.sabio_df[\"Enzymename\"].unique().tolist()\n",
    "        for enzyme in enzymes:\n",
    "            print('enzyme', enzyme)\n",
    "            enzyme_df = self.sabio_df.loc[self.sabio_df[\"Enzymename\"] == enzyme]\n",
    "            enzyme_dict[enzyme] = {}\n",
    "            reactions = enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                enzyme_dict[enzyme][reaction] = {}\n",
    "                \n",
    "                # ensure that the reaction chemicals match before accepting kinetic data\n",
    "                print('reaction', reaction)\n",
    "                rxn_string, sabio_chemicals, expected_bigg_chemicals= self._split_reaction(reaction, sabio = True)\n",
    "                bigg_chemicals = self.model_contents[enzyme]['bigg_compounds']\n",
    "                \n",
    "                extra_bigg = set(bigg_chemicals) - set(expected_bigg_chemicals) \n",
    "                extra_bigg = set(re.sub('(H\\+|H2O)', '', chem) for chem in extra_bigg)           \n",
    "                if len(extra_bigg) != 1:\n",
    "                    missed_reaction = f'The || {rxn_string} || reaction with {expected_bigg_chemicals} chemicals does not match the BiGG reaction of {bigg_chemicals} chemicals.'\n",
    "                    if self.printing:\n",
    "                        print(missed_reaction)\n",
    "                    enzyme_dict[enzyme][reaction] = missed_reaction\n",
    "                    continue\n",
    "                \n",
    "                # parse each entryid of each reaction\n",
    "                enzyme_reactions_df = enzyme_df.loc[enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = enzyme_reactions_df[\"EntryID\"].unique().tolist()\n",
    "                for entryid in entryids:\n",
    "                    enzyme_reaction_entryids_df = enzyme_reactions_df.loc[enzyme_reactions_df[\"EntryID\"] == entryid]\n",
    "                    entryid_string = f'condition_{entryid}'\n",
    "                    enzyme_dict[enzyme][reaction][entryid_string] = {}\n",
    "                    head_of_df = enzyme_reaction_entryids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        enzyme_dict[enzyme][reaction][entryid_string][\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        enzyme_dict[enzyme][reaction][entryid_string][\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        enzyme_dict[enzyme][reaction][entryid_string][\"RateLaw\"] = rate_law\n",
    "                        enzyme_dict[enzyme][reaction][entryid_string][\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        enzyme_dict[enzyme][reaction][entryid_string][\"RateLaw\"] = \"NaN\"\n",
    "                        enzyme_dict[enzyme][reaction][entryid_string][\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            enzyme_dict[enzyme][reaction][entryid_string][field] = head_of_df[field]\n",
    "                            \n",
    "                        enzyme_dict[enzyme][reaction][entryid_string][\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":           # The quantities must be converted to base units\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            enzyme_dict[enzyme][reaction][entryid_string][\"RateLawSubstrates\"] = substrates_key\n",
    "                            enzyme_dict[enzyme][reaction][entryid_string][\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "        with open(self.paths[\"scraped_model_path\"], 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(enzyme_dict, f, indent=4, sort_keys=True, separators=(', ', ': '), ensure_ascii=False, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enzyme 6-phosphogluconolactonase\n",
      "reaction H2O + 6-Phospho-D-glucono-1,5-lactone = 6-Phospho-D-gluconate\n",
      "The || H2O H2O + 6-Phospho-D-glucono-1,5-lactone <-> 6-Phospho-D-gluconate || reaction with ['H2O H2O', '6-Phospho-D-glucono-1,5-lactone', '6-Phospho-D-gluconate'] chemicals does not match the BiGG reaction of ['6-phospho-D-glucono-1,5-lactone', 'H2O', '6-Phospho-D-gluconate', 'H+'] chemicals.\n",
      "enzyme  \n",
      "reaction Formate = Formate\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "' '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-972dfbcdf911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-3da7c77d1896>\u001b[0m in \u001b[0;36mcombine_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reaction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreaction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mrxn_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msabio_chemicals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_bigg_chemicals\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_reaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreaction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msabio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[0mbigg_chemicals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menzyme\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'chemicals'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mextra_bigg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigg_chemicals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_bigg_chemicals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "tes = test()\n",
    "tes.combine_data()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

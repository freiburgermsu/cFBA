{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,):\n",
    "        global scraped_entryids\n",
    "        global entry_id_json_out\n",
    "\n",
    "        # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "            \n",
    "        progress_file_prefix = \"current-progress\"\n",
    "        xls_download_prefix = \"xls-download-\"\n",
    "        scraped_xls_prefix = \"scraped-xls-\"\n",
    "        scraped_entryids_prefix = \"scraped-entryids-\"\n",
    "        processed_xls = \"proccessed-xls-\"\n",
    "        entry_json = \"entryids-json-\"\n",
    "        scraped_model = \"scraped-model-\"\n",
    "        \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], scraped_model) + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],xls_download_prefix)\n",
    "        \n",
    "        self.step_number = -1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], progress_file_prefix) + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not self.step_number in [1, 2, 3, 4, 5]:\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], xls_download_prefix) \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], scraped_xls_prefix) + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], scraped_entryids_prefix) + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], entry_json) + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        global enzymes\n",
    "        global xls_download_path\n",
    "\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)    \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            driver.close()\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "                                try:\n",
    "                                    success_flag = self.variables['scraped_xls'](id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    try:\n",
    "                        success_flag = self.variables['scraped_xls'](reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path'])))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], processed_xls) + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(step):\n",
    "        if not re.search('[0-5]', step):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the BIGG Model JSON file path: C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_models\\BiGG model of S. aureus.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1cb1fbd9dcd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_models\\BiGG model of S. aureus.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscraping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSABIO_scraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscraping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e76fec7a5637>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_bigg_xls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob_xls_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_entryids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e76fec7a5637>\u001b[0m in \u001b[0;36mglob_xls_files\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;31m# combine the total set of dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_dataframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;31m# replace the NaN values with blank spaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_models\\BiGG model of S. aureus.json\n",
    "scraping = SABIO_scraping()\n",
    "scraping.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

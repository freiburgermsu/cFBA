{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "        driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        if source == 'sabio':\n",
    "            driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,):\n",
    "        # find the BiGG model that will be scraped\n",
    "#         while True:\n",
    "#             bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "#             if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "#                 try:\n",
    "#                     self.model = json.load(open(bigg_model_path))\n",
    "#                     bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "#                     break\n",
    "#                 except:\n",
    "#                     pass\n",
    "\n",
    "        self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "        bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "        bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count)\n",
    "        \n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)    \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "                                success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "#                                 try:\n",
    "#                                     success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "#                                 except:\n",
    "#                                     success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "#                     try:\n",
    "#                         success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "#                     except:\n",
    "#                         success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\dFBApy\\scraping\\SABIO\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-c1b885fd34c3>\u001b[0m in \u001b[0;36mscrape_xls\u001b[1;34m(self, reaction_identifier, search_option)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mresult_num_ele\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"numberofKinLaw\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_num_ele\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_id\u001b[1;34m(self, id_)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"numberofKinLaw\"]\"}\n  (Session info: chrome=93.0.4577.82)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-cebf96d75e31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscraping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSABIO_scraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscraping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-c1b885fd34c3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_bigg_xls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob_xls_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-c1b885fd34c3>\u001b[0m in \u001b[0;36mscrape_bigg_xls\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuccess_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                     \u001b[0msuccess_flag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_xls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreaction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Enzymename\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;31m#                     try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;31m#                         success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-c1b885fd34c3>\u001b[0m in \u001b[0;36mscrape_xls\u001b[1;34m(self, reaction_identifier, search_option)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0mresult_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "# C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_models\\BiGG model of S. aureus.json\n",
    "# ./BiGG_models/BiGG model of S. aureus.json\n",
    "\n",
    "scraping = SABIO_scraping()\n",
    "scraping.main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
